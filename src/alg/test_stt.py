import contextlib
import time

import numpy as np
import sounddevice as sd
from faster_whisper import WhisperModel

# Whisperãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆRaspberry Pi 5 ã® CPU ç”¨ã« int8 è¨­å®šï¼‰
# model = WhisperModel("large-v3-turbo", device="cpu", compute_type="int8")
model = WhisperModel("tiny", device="cpu")

# USBãƒã‚¤ã‚¯ã®ãƒ‡ãƒã‚¤ã‚¹ç•ªå·ã‚’æŒ‡å®šï¼ˆarecord -l ã§ç¢ºèªï¼‰
DEVICE_INDEX = 3  # C970 USBãƒã‚¤ã‚¯
SAMPLE_RATE = 16000  # Whisper ãŒæœŸå¾…ã™ã‚‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
BUFFER_DURATION = 3  # 5ç§’ã”ã¨ã«æ–‡å­—èµ·ã“ã—

# ãƒã‚¤ã‚ºã‚’æ¤œå‡ºã™ã‚‹ãŸã‚ã®ã—ãã„å€¤
THRESHOLD = 0.01  # ä¾‹ãˆã°ã€0.01ä»¥ä¸‹ã®æŒ¯å¹…ã¯ç„¡è¦–


@contextlib.contextmanager
def timer():
    """å‡¦ç†æ™‚é–“ã‚’è¨ˆæ¸¬ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£"""
    start = time.monotonic()
    yield
    end = time.monotonic()
    print(f"â±ï¸ æ–‡å­—èµ·ã“ã—æ™‚é–“: {end - start:.3f} ç§’")


def callback(indata, frames, time, status):
    """éŸ³å£°å…¥åŠ›ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒ•ã‚¡ã«æ ¼ç´ï¼‰"""
    if status:
        print(f"âš ï¸ Sounddevice status: {status}")
    # ãƒã‚¤ã‚ºã—ãã„å€¤ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    filtered_data = indata[:, 0]
    if np.max(np.abs(filtered_data)) > THRESHOLD:
        audio_buffer.extend(filtered_data)  # ãƒ¢ãƒãƒ©ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ã¿å–å¾—


if __name__ == "__main__":
    audio_buffer = []

    print("ğŸ¤ éŸ³å£°æ–‡å­—èµ·ã“ã—ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆUSBãƒã‚¤ã‚¯ï¼‰")
    with sd.InputStream(
        samplerate=SAMPLE_RATE,
        channels=1,
        dtype=np.int16,
        callback=callback,
        device=DEVICE_INDEX,
    ):
        while True:
            try:
                if len(audio_buffer) >= SAMPLE_RATE * BUFFER_DURATION:
                    with timer():
                        # int16 -> float32 å¤‰æ›
                        audio_data = (
                            np.array(audio_buffer, dtype=np.int16).astype(np.float32)
                            / 32768.0
                        )
                        audio_buffer = []  # ãƒãƒƒãƒ•ã‚¡ã‚’ãƒªã‚»ãƒƒãƒˆ

                        # æ–‡å­—èµ·ã“ã—å‡¦ç†æ™‚é–“ã‚’è¨ˆæ¸¬
                        segments, _ = model.transcribe(audio_data, language="ja")

                    text = "".join(segment.text for segment in segments)
                    print(f"ğŸ“ æ–‡å­—èµ·ã“ã—çµæœ: {text}")

            except KeyboardInterrupt:
                print("\nğŸ›‘ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
                break
            except Exception as e:
                print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}")
